{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daonly/2023Autumn/blob/main/2022021241_%EC%9D%B4%EB%8B%A4%EC%98%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "565d8a97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "565d8a97",
        "outputId": "a179e310-bf04-4939-ad44-0bce9e123404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive # 코랩과 구글드라이드를 연동해줍니다.\n",
        "drive.mount('/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3a600b39",
      "metadata": {
        "id": "3a600b39"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/gdrive/My Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "008cf1ad",
      "metadata": {
        "id": "008cf1ad"
      },
      "source": [
        "## Preliminary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "595d3c8f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:58:06.522445Z",
          "start_time": "2023-12-08T13:58:06.227446Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "595d3c8f",
        "outputId": "8b49da5c-cdc5-4bbc-cf2e-70cb82f982a4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ae48bb0fed57>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfashion_mnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_fashion_mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcifar_10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_cifar_10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset.fashion_mnist'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "np.random.seed(1211)\n",
        "\n",
        "import time\n",
        "\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "\n",
        "from dataset.mnist import load_mnist\n",
        "from dataset.fashion_mnist import load_fashion_mnist\n",
        "from dataset.cifar_10 import load_cifar_10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5998a30",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:58:06.538446Z",
          "start_time": "2023-12-08T13:58:06.523446Z"
        },
        "id": "a5998a30"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 곱셈 계층\n",
        "# =============================================================================\n",
        "class MulLayer:\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        out = x * y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * self.y  # x와 y를 바꾼다.\n",
        "        dy = dout * self.x\n",
        "\n",
        "        return dx, dy\n",
        "\n",
        "# =============================================================================\n",
        "# 덧셈 계층\n",
        "# =============================================================================\n",
        "class AddLayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = x + y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * 1\n",
        "        dy = dout * 1\n",
        "\n",
        "        return dx, dy\n",
        "\n",
        "# =============================================================================\n",
        "# ReLU 계층\n",
        "# =============================================================================\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "# =============================================================================\n",
        "# Sigmoid 계층\n",
        "# =============================================================================\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x) # 순전파의 출력을 out에 보관한 후 역전파 계산 때 그 값을 사용\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "# =============================================================================\n",
        "# Affine 계층\n",
        "# =============================================================================\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx\n",
        "\n",
        "# =============================================================================\n",
        "# Softmax-with-Loss 계층\n",
        "# =============================================================================\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "\n",
        "        return dx\n",
        "\n",
        "# =============================================================================\n",
        "# 오차역전파를 적용한 신경망\n",
        "# =============================================================================\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb80e8b",
      "metadata": {
        "id": "9bb80e8b"
      },
      "source": [
        "### 오차역전차법을 사용한 학습 예시"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5be32e3",
      "metadata": {
        "id": "a5be32e3"
      },
      "outputs": [],
      "source": [
        "# # 데이터 읽기\n",
        "# (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "# network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "# iters_num = 10000\n",
        "# train_size = x_train.shape[0]\n",
        "# batch_size = 100\n",
        "# learning_rate = 0.1\n",
        "\n",
        "# train_loss_list = []\n",
        "# train_acc_list = []\n",
        "# test_acc_list = []\n",
        "\n",
        "# iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "# for i in range(iters_num):\n",
        "#     batch_mask = np.random.choice(train_size, batch_size)\n",
        "#     x_batch = x_train[batch_mask]\n",
        "#     t_batch = t_train[batch_mask]\n",
        "\n",
        "#     # 기울기 계산\n",
        "#     #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "#     grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "#     # 갱신\n",
        "#     for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "#         network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "#     loss = network.loss(x_batch, t_batch)\n",
        "#     train_loss_list.append(loss)\n",
        "\n",
        "#     if i % iter_per_epoch == 0:\n",
        "#         train_acc = network.accuracy(x_train, t_train)\n",
        "#         test_acc = network.accuracy(x_test, t_test)\n",
        "#         train_acc_list.append(train_acc)\n",
        "#         test_acc_list.append(test_acc)\n",
        "#         print(train_acc, test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccd590a6",
      "metadata": {
        "id": "ccd590a6"
      },
      "source": [
        "# Q1. 제공된 데이터 세트로 변경하여 다음 코드를 실행하고, 결과를 저장하시오."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e333af6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:58:31.258386Z",
          "start_time": "2023-12-08T13:58:06.539446Z"
        },
        "id": "9e333af6"
      },
      "outputs": [],
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_fashion_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3298d5a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:58:52.155977Z",
          "start_time": "2023-12-08T13:58:31.259387Z"
        },
        "id": "a3298d5a"
      },
      "outputs": [],
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_cifar_10(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=1024, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f217a667",
      "metadata": {
        "id": "f217a667"
      },
      "source": [
        "# Q2. 신경망 학습에서 학습하는데 한 번에 쓸 수 있는 데이터의 크기를 결정하는 'batch size'와 weight를 업데이트하는데 사용되는 'learning rate'는 학습 결과에 영향을 주는 중요한 하이퍼 파라미터이다. 다음 [batch_size, learning_rate] 가운데 1개를 선택해 수정하고, 수정 전 코드의 실행 결과와 비교하여 발생되는 차이점을 작성하시오."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d31cd944",
      "metadata": {
        "id": "d31cd944"
      },
      "source": [
        "## 변경 전 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c03faa7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:59:01.194573Z",
          "start_time": "2023-12-08T13:58:52.156976Z"
        },
        "id": "1c03faa7"
      },
      "outputs": [],
      "source": [
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db4ee84",
      "metadata": {
        "id": "2db4ee84"
      },
      "source": [
        "## 변경 후 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ef8bf5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:59:10.006218Z",
          "start_time": "2023-12-08T13:59:01.195574Z"
        },
        "id": "63ef8bf5"
      },
      "outputs": [],
      "source": [
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100                                             # <== 이 부분을 수정하면 됨\n",
        "learning_rate = 0.01                                             # <== 이 부분을 수정하면 됨\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa137fde",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:59:10.022218Z",
          "start_time": "2023-12-08T13:59:10.007218Z"
        },
        "id": "fa137fde"
      },
      "outputs": [],
      "source": [
        "print(\"____________________________________________________________________________________\")       # <= 이 부분에 답안을 작성"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92d2c141",
      "metadata": {
        "id": "92d2c141"
      },
      "source": [
        "# Q3. 기울기 계산 방법에 대해 '수치 미분 방식'과 '오차역전파 방식'을 개별 실험하고, 계산 소요 시간 차이에 대한 원인과 이유를 작성하시오.\n",
        "\t# Tips: 이 문제는 코드의 실행 여부를 확인하며, 본인의 생각을 자유롭게 작성하시면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50fbeb7c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T14:01:05.360332Z",
          "start_time": "2023-12-08T13:59:10.023218Z"
        },
        "id": "50fbeb7c"
      },
      "outputs": [],
      "source": [
        "# 코드 실행 전 시간 기록\n",
        "start_time = time.time()\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식                                             # <== 이 부분을 수정하면 됨\n",
        "#   grad = __________________________________________ # 오차역전파법 방식                                             # <== 이 부분을 수정하면 됨\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "# if i % iter_per_epoch == 0:\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(train_acc, test_acc, \"--\" \"Itertation_number =\", i)\n",
        "\n",
        "# 코드 실행 후 시간 기록\n",
        "end_time = time.time()\n",
        "\n",
        "# 실행 시간 계산\n",
        "execution_time = end_time - start_time\n",
        "print(f\"수치 미분 방식 코드 실행 시간: {execution_time} 초\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed88b5ae",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T14:01:14.290416Z",
          "start_time": "2023-12-08T14:01:05.362332Z"
        },
        "id": "ed88b5ae"
      },
      "outputs": [],
      "source": [
        "# 코드 실행 전 시간 기록\n",
        "start_time = time.time()\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "#   grad = __________________________________________ # 수치 미분 방식                                             # <== 이 부분을 수정하면 됨\n",
        "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식                                             # <== 이 부분을 수정하면 됨\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc, \"--\" \"Itertation_number =\", i)\n",
        "\n",
        "# 코드 실행 후 시간 기록\n",
        "end_time = time.time()\n",
        "\n",
        "# 실행 시간 계산\n",
        "execution_time = end_time - start_time\n",
        "print(f\"수치 미분 방식 코드 실행 시간: {execution_time} 초\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e1cd323",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:59:10.022218Z",
          "start_time": "2023-12-08T13:59:10.007218Z"
        },
        "id": "7e1cd323"
      },
      "outputs": [],
      "source": [
        "print(\"____________________________________________________________________________________\")       # <= 이 부분에 답안을 작성"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22acf981",
      "metadata": {
        "id": "22acf981"
      },
      "source": [
        "# Q4. 다음 class에서 ReLU 활성화 함수를 sigmoid로 변경하여 코드를 실행하고, 각 활성화 함수에 대한 특징을 작성하시오."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e2680a9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T14:01:14.306416Z",
          "start_time": "2023-12-08T14:01:14.291416Z"
        },
        "id": "7e2680a9"
      },
      "outputs": [],
      "source": [
        "class TwoLayerNet_Sigmoid:\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        _____________________________________________________________________                                             # <== 이 부분을 수정하면 됨\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf3a8770",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T14:01:23.165463Z",
          "start_time": "2023-12-08T14:01:14.307416Z"
        },
        "id": "bf3a8770"
      },
      "outputs": [],
      "source": [
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet_Sigmoid(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd7f2b85",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:59:10.022218Z",
          "start_time": "2023-12-08T13:59:10.007218Z"
        },
        "id": "cd7f2b85"
      },
      "outputs": [],
      "source": [
        "print(\"____________________________________________________________________________________\")       # <= 이 부분에 답안을 작성"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2be92bf6",
      "metadata": {
        "id": "2be92bf6"
      },
      "source": [
        "# Q5. 다음 class에서 Affine layer를 한 층 더 추가하여 코드를 실행하고, 이에 따른 결과를 기존 실행 결과와 비교하시오.\n",
        "    (두 번째 활성화 함수도 동일하게 ReLU를 사용할 것)\n",
        "    (첫 번째 hidden layer의 size는 100, 두 번째 hidden layer의 size는 50으로 설정할 것)\n",
        "\t# Tip: 제공된 code의 주석 부분에 대해 layer를 추가함으로써 Weight 파라미터에 대해 생각하면 좋을 것 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d78b083",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T14:01:23.181463Z",
          "start_time": "2023-12-08T14:01:23.166463Z"
        },
        "id": "8d78b083"
      },
      "outputs": [],
      "source": [
        "class ThreeLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, ________________, ______________, output_size, weight_init_std = 0.01):        # <== 이 부분을 수정 또는 추가하면 됨\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        ___________________________________________________________________________________      # <== 이 부분을 수정 또는 추가하면 됨\n",
        "        ___________________________________________                                              # <== 이 부분을 수정 또는 추가하면 됨\n",
        "        ___________________________________________________________________________________      # <== 이 부분을 수정 또는 추가하면 됨\n",
        "        ___________________________________________                                              # <== 이 부분을 수정 또는 추가하면 됨\n",
        "        ___________________________________________________________________________________      # <== 이 부분을 수정 또는 추가하면 됨\n",
        "        ___________________________________________                                              # <== 이 부분을 수정 또는 추가하면 됨\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        _____________________________________________________________________                    # <== 이 부분을 수정 또는 추가하면 됨\n",
        "        _____________________________________________________________________                    # <== 이 부분을 수정 또는 추가하면 됨\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        ___________________________________________________________                    # <== 이 부분을 수정 또는 추가하면 됨\n",
        "        ___________________________________________________________                    # <== 이 부분을 수정 또는 추가하면 됨\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "        _______________________________________________________________________________                 # <== 이 부분을 수정 또는 추가하면 됨\n",
        "\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f89d381e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T14:01:37.488262Z",
          "start_time": "2023-12-08T14:01:23.182463Z"
        },
        "id": "f89d381e"
      },
      "outputs": [],
      "source": [
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = ThreeLayerNet(input_size=784, ____________=_____, ____________=_____, output_size=10) # <== 이 부분을 수정 또는 추가하면 됨\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "    # 갱신\n",
        "    for key in (_________________________________________):                      # <== 이 부분을 수정 또는 추가하면 됨\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e614509",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:59:10.022218Z",
          "start_time": "2023-12-08T13:59:10.007218Z"
        },
        "id": "0e614509"
      },
      "outputs": [],
      "source": [
        "print(\"____________________________________________________________________________________\")       # <= 이 부분에 답안을 작성"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "308.965px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}