{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daonly/2023Autumn/blob/main/2022021241_%EC%9D%B4%EB%8B%A4%EC%98%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "565d8a97",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "565d8a97",
        "outputId": "5abff507-6fcc-4935-d7b9-6552373b0657"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive # 코랩과 구글드라이드를 연동해줍니다.\n",
        "drive.mount('/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3a600b39",
      "metadata": {
        "id": "3a600b39"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/gdrive/My Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "008cf1ad",
      "metadata": {
        "id": "008cf1ad"
      },
      "source": [
        "## Preliminary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "595d3c8f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:58:06.522445Z",
          "start_time": "2023-12-08T13:58:06.227446Z"
        },
        "id": "595d3c8f"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "np.random.seed(1211)\n",
        "\n",
        "import time\n",
        "\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "\n",
        "from dataset.mnist import load_mnist\n",
        "from dataset.fashion_mnist import load_fashion_mnist\n",
        "from dataset.cifar_10 import load_cifar_10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a5998a30",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:58:06.538446Z",
          "start_time": "2023-12-08T13:58:06.523446Z"
        },
        "id": "a5998a30"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 곱셈 계층\n",
        "# =============================================================================\n",
        "class MulLayer:\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        out = x * y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * self.y  # x와 y를 바꾼다.\n",
        "        dy = dout * self.x\n",
        "\n",
        "        return dx, dy\n",
        "\n",
        "# =============================================================================\n",
        "# 덧셈 계층\n",
        "# =============================================================================\n",
        "class AddLayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = x + y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * 1\n",
        "        dy = dout * 1\n",
        "\n",
        "        return dx, dy\n",
        "\n",
        "# =============================================================================\n",
        "# ReLU 계층\n",
        "# =============================================================================\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "# =============================================================================\n",
        "# Sigmoid 계층\n",
        "# =============================================================================\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x) # 순전파의 출력을 out에 보관한 후 역전파 계산 때 그 값을 사용\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "# =============================================================================\n",
        "# Affine 계층\n",
        "# =============================================================================\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 편향 매개변수의 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
        "        return dx\n",
        "\n",
        "# =============================================================================\n",
        "# Softmax-with-Loss 계층\n",
        "# =============================================================================\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실함수\n",
        "        self.y = None    # softmax의 출력\n",
        "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "\n",
        "        return dx\n",
        "\n",
        "# =============================================================================\n",
        "# 오차역전파를 적용한 신경망\n",
        "# =============================================================================\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb80e8b",
      "metadata": {
        "id": "9bb80e8b"
      },
      "source": [
        "### 오차역전차법을 사용한 학습 예시"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5be32e3",
      "metadata": {
        "id": "a5be32e3"
      },
      "outputs": [],
      "source": [
        "# # 데이터 읽기\n",
        "# (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "# network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "# iters_num = 10000\n",
        "# train_size = x_train.shape[0]\n",
        "# batch_size = 100\n",
        "# learning_rate = 0.1\n",
        "\n",
        "# train_loss_list = []\n",
        "# train_acc_list = []\n",
        "# test_acc_list = []\n",
        "\n",
        "# iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "# for i in range(iters_num):\n",
        "#     batch_mask = np.random.choice(train_size, batch_size)\n",
        "#     x_batch = x_train[batch_mask]\n",
        "#     t_batch = t_train[batch_mask]\n",
        "\n",
        "#     # 기울기 계산\n",
        "#     #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "#     grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "#     # 갱신\n",
        "#     for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "#         network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "#     loss = network.loss(x_batch, t_batch)\n",
        "#     train_loss_list.append(loss)\n",
        "\n",
        "#     if i % iter_per_epoch == 0:\n",
        "#         train_acc = network.accuracy(x_train, t_train)\n",
        "#         test_acc = network.accuracy(x_test, t_test)\n",
        "#         train_acc_list.append(train_acc)\n",
        "#         test_acc_list.append(test_acc)\n",
        "#         print(train_acc, test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccd590a6",
      "metadata": {
        "id": "ccd590a6"
      },
      "source": [
        "# Q1. 제공된 데이터 세트로 변경하여 다음 코드를 실행하고, 결과를 저장하시오."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e333af6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:58:31.258386Z",
          "start_time": "2023-12-08T13:58:06.539446Z"
        },
        "id": "9e333af6",
        "outputId": "1291cc5e-c499-4118-ac05-270a36c5e8f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train has changed completely\n",
            "x_test has changed completely\n",
            "t_train has changed completely\n",
            "t_test has changed completely\n",
            "0.1837 0.1869\n",
            "0.81174 0.8094\n",
            "0.82772 0.823\n",
            "0.84482 0.8392\n",
            "0.84274 0.8394\n",
            "0.85772 0.8496\n",
            "0.86342 0.8564\n",
            "0.868 0.8585\n",
            "0.87596 0.8666\n",
            "0.87578 0.8662\n",
            "0.87534 0.8654\n",
            "0.8736 0.8629\n",
            "0.88372 0.871\n",
            "0.88658 0.8734\n",
            "0.88778 0.8711\n",
            "0.88612 0.8733\n",
            "0.89338 0.876\n",
            "0.89288 0.8768\n",
            "0.892 0.8736\n",
            "0.89516 0.8759\n"
          ]
        }
      ],
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_fashion_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3298d5a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:58:52.155977Z",
          "start_time": "2023-12-08T13:58:31.259387Z"
        },
        "id": "a3298d5a",
        "outputId": "1d826fbb-f4e8-4944-80ac-43ce3a8d98e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train has changed completely\n",
            "x_test has changed completely\n",
            "t_train has changed completely\n",
            "t_test has changed completely\n",
            "0.0985 0.0987\n",
            "0.31795 0.3099\n",
            "0.369475 0.3578\n",
            "0.374125 0.3593\n",
            "0.400275 0.3892\n",
            "0.433625 0.4184\n",
            "0.388375 0.3738\n",
            "0.419775 0.4079\n",
            "0.449375 0.4321\n",
            "0.4239 0.4051\n",
            "0.466425 0.4416\n",
            "0.46185 0.4379\n",
            "0.47985 0.459\n",
            "0.4776 0.4556\n",
            "0.4786 0.4496\n",
            "0.46995 0.4498\n",
            "0.48915 0.4668\n",
            "0.485525 0.4642\n",
            "0.481975 0.4518\n",
            "0.46505 0.4413\n",
            "0.4957 0.4677\n",
            "0.5072 0.4762\n",
            "0.47445 0.4447\n",
            "0.4799 0.4484\n",
            "0.509075 0.4747\n"
          ]
        }
      ],
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_cifar_10(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=1024, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f217a667",
      "metadata": {
        "id": "f217a667"
      },
      "source": [
        "# Q2. 신경망 학습에서 학습하는데 한 번에 쓸 수 있는 데이터의 크기를 결정하는 'batch size'와 weight를 업데이트하는데 사용되는 'learning rate'는 학습 결과에 영향을 주는 중요한 하이퍼 파라미터이다. 다음 [batch_size, learning_rate] 가운데 1개를 선택해 수정하고, 수정 전 코드의 실행 결과와 비교하여 발생되는 차이점을 작성하시오."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d31cd944",
      "metadata": {
        "id": "d31cd944"
      },
      "source": [
        "## 변경 전 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c03faa7",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:59:01.194573Z",
          "start_time": "2023-12-08T13:58:52.156976Z"
        },
        "id": "1c03faa7",
        "outputId": "e0b90bea-5ab3-4ac7-e365-71f8f2775860",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09375 0.0941\n",
            "0.9042333333333333 0.9089\n",
            "0.9238833333333333 0.9264\n",
            "0.9362333333333334 0.9364\n",
            "0.9447833333333333 0.9436\n",
            "0.9501333333333334 0.9473\n",
            "0.9552333333333334 0.9526\n",
            "0.9592833333333334 0.956\n",
            "0.9641166666666666 0.9612\n",
            "0.9671166666666666 0.9625\n",
            "0.9693 0.9653\n",
            "0.97195 0.9676\n",
            "0.9728833333333333 0.9665\n",
            "0.9761 0.9688\n",
            "0.97695 0.9694\n",
            "0.9786333333333334 0.9714\n",
            "0.979 0.9718\n"
          ]
        }
      ],
      "source": [
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2db4ee84",
      "metadata": {
        "id": "2db4ee84"
      },
      "source": [
        "## 변경 후 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63ef8bf5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:59:10.006218Z",
          "start_time": "2023-12-08T13:59:01.195574Z"
        },
        "id": "63ef8bf5",
        "outputId": "f3f0829d-f680-485c-c512-7f62236c64fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.032466666666666665 0.0314\n",
            "0.5902166666666666 0.5949\n",
            "0.7792333333333333 0.7847\n",
            "0.8386333333333333 0.845\n",
            "0.86575 0.872\n",
            "0.8796833333333334 0.8846\n",
            "0.8889 0.8927\n",
            "0.8949 0.8983\n",
            "0.8989833333333334 0.9024\n",
            "0.90295 0.9057\n",
            "0.90585 0.9099\n",
            "0.9082166666666667 0.9117\n",
            "0.9105 0.9142\n",
            "0.91245 0.9169\n",
            "0.9138666666666667 0.9156\n",
            "0.91555 0.9193\n",
            "0.9175333333333333 0.9202\n"
          ]
        }
      ],
      "source": [
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100                                             # <== 이 부분을 수정하면 됨\n",
        "learning_rate = 0.01                                             # <== 이 부분을 수정하면 됨\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa137fde",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:59:10.022218Z",
          "start_time": "2023-12-08T13:59:10.007218Z"
        },
        "id": "fa137fde",
        "outputId": "20996947-b4d3-4c38-daca-37d0437a2ccc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning rate을 10분의 1로 줄여서 비교해보았다. 확실히 같은 횟수의 iteration을 돌았을 때 Acc 값의 차이로 보아, 학습이 더디게 진행되는 것을 확인할 수 있었다.\n"
          ]
        }
      ],
      "source": [
        "print(\"learning rate을 10분의 1로 줄여서 비교해보았다. 확실히 같은 횟수의 iteration을 돌았을 때 Acc 값의 차이로 보아, 학습이 더디게 진행되는 것을 확인할 수 있었다.\")       # <= 이 부분에 답안을 작성"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92d2c141",
      "metadata": {
        "id": "92d2c141"
      },
      "source": [
        "# Q3. 기울기 계산 방법에 대해 '수치 미분 방식'과 '오차역전파 방식'을 개별 실험하고, 계산 소요 시간 차이에 대한 원인과 이유를 작성하시오.\n",
        "\t# Tips: 이 문제는 코드의 실행 여부를 확인하며, 본인의 생각을 자유롭게 작성하시면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50fbeb7c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T14:01:05.360332Z",
          "start_time": "2023-12-08T13:59:10.023218Z"
        },
        "id": "50fbeb7c",
        "outputId": "e0c7c460-af80-4566-dac4-5d9e4154e705",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.08618333333333333 0.0868 --Itertation_number = 0\n",
            "0.11161666666666667 0.1137 --Itertation_number = 1\n",
            "0.11748333333333333 0.1215 --Itertation_number = 2\n",
            "0.1038 0.1028 --Itertation_number = 3\n",
            "0.10253333333333334 0.1016 --Itertation_number = 4\n",
            "0.1258 0.1236 --Itertation_number = 5\n",
            "0.1195 0.1188 --Itertation_number = 6\n",
            "0.10461666666666666 0.1039 --Itertation_number = 7\n",
            "0.10608333333333334 0.1056 --Itertation_number = 8\n",
            "0.13701666666666668 0.1332 --Itertation_number = 9\n",
            "수치 미분 방식 코드 실행 시간: 844.1539824008942 초\n"
          ]
        }
      ],
      "source": [
        "# 코드 실행 전 시간 기록\n",
        "start_time = time.time()\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식                                             # <== 이 부분을 수정하면 됨\n",
        "#   grad = __________________________________________ # 오차역전파법 방식                                             # <== 이 부분을 수정하면 됨\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "# if i % iter_per_epoch == 0:\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(train_acc, test_acc, \"--\" \"Itertation_number =\", i)\n",
        "\n",
        "# 코드 실행 후 시간 기록\n",
        "end_time = time.time()\n",
        "\n",
        "# 실행 시간 계산\n",
        "execution_time = end_time - start_time\n",
        "print(f\"수치 미분 방식 코드 실행 시간: {execution_time} 초\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "ed88b5ae",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T14:01:14.290416Z",
          "start_time": "2023-12-08T14:01:05.362332Z"
        },
        "id": "ed88b5ae",
        "outputId": "4011418d-63c0-4a23-c888-51ddde6ac308",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1752 0.18 --Itertation_number = 0\n",
            "0.8999833333333334 0.9044 --Itertation_number = 600\n",
            "0.9197 0.9221 --Itertation_number = 1200\n",
            "0.9321333333333334 0.9354 --Itertation_number = 1800\n",
            "0.9412166666666667 0.9409 --Itertation_number = 2400\n",
            "0.949 0.9467 --Itertation_number = 3000\n",
            "0.9543 0.952 --Itertation_number = 3600\n",
            "0.95765 0.9548 --Itertation_number = 4200\n",
            "0.9634333333333334 0.9599 --Itertation_number = 4800\n",
            "0.9656666666666667 0.9593 --Itertation_number = 5400\n",
            "0.9687 0.9632 --Itertation_number = 6000\n",
            "0.9706666666666667 0.9652 --Itertation_number = 6600\n",
            "0.97305 0.9667 --Itertation_number = 7200\n",
            "0.9748166666666667 0.9674 --Itertation_number = 7800\n",
            "0.97695 0.9688 --Itertation_number = 8400\n",
            "0.9772666666666666 0.9691 --Itertation_number = 9000\n",
            "0.9797666666666667 0.9701 --Itertation_number = 9600\n",
            "오차역전파 코드 실행 시간: 27.092668294906616 초\n"
          ]
        }
      ],
      "source": [
        "# 코드 실행 전 시간 기록\n",
        "start_time = time.time()\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "#   grad = __________________________________________ # 수치 미분 방식                                             # <== 이 부분을 수정하면 됨\n",
        "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식                                             # <== 이 부분을 수정하면 됨\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc, \"--\" \"Itertation_number =\", i)\n",
        "\n",
        "# 코드 실행 후 시간 기록\n",
        "end_time = time.time()\n",
        "\n",
        "# 실행 시간 계산\n",
        "execution_time = end_time - start_time\n",
        "print(f\"오차역전파 코드 실행 시간: {execution_time} 초\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7e1cd323",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:59:10.022218Z",
          "start_time": "2023-12-08T13:59:10.007218Z"
        },
        "id": "7e1cd323",
        "outputId": "f1272022-755a-4b88-ef86-11248c6d582f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확실히 수치 미분 방식에 비해 오차역전파 방식이 미분을 효울적으로 할 수 있기 때문에 훨씬 빠르게 결과를 내는 것을 확인할 수 있었다.\n"
          ]
        }
      ],
      "source": [
        "print(\"확실히 수치 미분 방식에 비해 오차역전파 방식이 미분을 효울적으로 할 수 있기 때문에 훨씬 빠르게 결과를 내는 것을 확인할 수 있었다.\")       # <= 이 부분에 답안을 작성"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22acf981",
      "metadata": {
        "id": "22acf981"
      },
      "source": [
        "# Q4. 다음 class에서 ReLU 활성화 함수를 sigmoid로 변경하여 코드를 실행하고, 각 활성화 함수에 대한 특징을 작성하시오."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7e2680a9",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T14:01:14.306416Z",
          "start_time": "2023-12-08T14:01:14.291416Z"
        },
        "id": "7e2680a9"
      },
      "outputs": [],
      "source": [
        "class TwoLayerNet_Sigmoid:\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Sigmoid1'] = Sigmoid()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bf3a8770",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T14:01:23.165463Z",
          "start_time": "2023-12-08T14:01:14.307416Z"
        },
        "id": "bf3a8770",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "094174c8-99fe-442c-9a7b-97a0ba626b0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09751666666666667 0.0974\n",
            "0.773 0.7832\n",
            "0.87595 0.8806\n",
            "0.89725 0.9014\n",
            "0.9068 0.911\n",
            "0.9127 0.9155\n",
            "0.9189166666666667 0.9222\n",
            "0.9232166666666667 0.9258\n",
            "0.927 0.929\n",
            "0.93115 0.9316\n",
            "0.9346166666666667 0.9346\n",
            "0.9366833333333333 0.9362\n",
            "0.9395666666666667 0.94\n",
            "0.9403833333333333 0.9405\n",
            "0.9432 0.9433\n",
            "0.9450166666666666 0.9449\n",
            "0.9466 0.9444\n"
          ]
        }
      ],
      "source": [
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet_Sigmoid(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cd7f2b85",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:59:10.022218Z",
          "start_time": "2023-12-08T13:59:10.007218Z"
        },
        "id": "cd7f2b85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b40a5ca5-ecdd-43d0-c03a-566b6d224266"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid의 vanishing gradient problem을 해결하기 위해 고안된 것이 ReLU이다. 다만 본인의 경우 이 코드에서는 ReLU보다 Sigmoid가 비슷한 소요 시간에 비해 더 정확한 퍼포먼스를 보여주는 것을 확인할 수 있었다. 따라서 본 데이터 분석에서는 vanishing gradient problem이 발생하지 않았던 것으로 사료된다.\n"
          ]
        }
      ],
      "source": [
        "print(\"Sigmoid의 vanishing gradient problem을 해결하기 위해 고안된 것이 ReLU이다. 다만 본인의 경우 이 코드에서는 ReLU보다 Sigmoid가 비슷한 소요 시간에 비해 더 정확한 퍼포먼스를 보여주는 것을 확인할 수 있었다. 따라서 본 데이터 분석에서는 vanishing gradient problem이 발생하지 않았던 것으로 사료된다.\")       # <= 이 부분에 답안을 작성"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2be92bf6",
      "metadata": {
        "id": "2be92bf6"
      },
      "source": [
        "# Q5. 다음 class에서 Affine layer를 한 층 더 추가하여 코드를 실행하고, 이에 따른 결과를 기존 실행 결과와 비교하시오.\n",
        "    (두 번째 활성화 함수도 동일하게 ReLU를 사용할 것)\n",
        "    (첫 번째 hidden layer의 size는 100, 두 번째 hidden layer의 size는 50으로 설정할 것)\n",
        "\t# Tip: 제공된 code의 주석 부분에 대해 layer를 추가함으로써 Weight 파라미터에 대해 생각하면 좋을 것 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8d78b083",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T14:01:23.181463Z",
          "start_time": "2023-12-08T14:01:23.166463Z"
        },
        "id": "8d78b083"
      },
      "outputs": [],
      "source": [
        "class ThreeLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, hidden_size2, output_size, weight_init_std = 0.01):        # <== 이 부분을 수정 또는 추가하면 됨\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, hidden_size2)\n",
        "        self.params['b2'] = np.zeros(hidden_size2)\n",
        "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size2, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        grads['W3'] = numerical_gradient(loss_W, self.params['W3'])\n",
        "        grads['b3'] = numerical_gradient(loss_W, self.params['b3'])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
        "\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f89d381e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T14:01:37.488262Z",
          "start_time": "2023-12-08T14:01:23.182463Z"
        },
        "id": "f89d381e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a990531-1eb8-4a9e-d726-6f2760d4a7af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09615 0.0932\n",
            "0.77895 0.7853\n",
            "0.9054166666666666 0.902\n",
            "0.9360833333333334 0.9336\n",
            "0.9516333333333333 0.947\n",
            "0.9616333333333333 0.9566\n",
            "0.9654333333333334 0.9608\n",
            "0.9723666666666667 0.9656\n",
            "0.9767333333333333 0.9695\n",
            "0.9773666666666667 0.9688\n",
            "0.9812833333333333 0.9707\n",
            "0.9840833333333333 0.9721\n",
            "0.9845333333333334 0.9718\n",
            "0.9851 0.9726\n",
            "0.9881333333333333 0.9756\n",
            "0.9881833333333333 0.9725\n",
            "0.9899333333333333 0.974\n"
          ]
        }
      ],
      "source": [
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = ThreeLayerNet(input_size=784, hidden_size=100, hidden_size2=50, output_size=10) # <== 이 부분을 수정 또는 추가하면 됨\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):                      # <== 이 부분을 수정 또는 추가하면 됨\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0e614509",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-12-08T13:59:10.022218Z",
          "start_time": "2023-12-08T13:59:10.007218Z"
        },
        "id": "0e614509",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4376b00-c3d5-499e-dec0-dd70176caafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer를 하나 더 늘리니 확실히 조금 더 오랜 시간이 소요되긴 하였으나, 더 높은 정확도를 확인할 수 있었다.\n"
          ]
        }
      ],
      "source": [
        "print(\"Layer를 하나 더 늘리니 확실히 조금 더 오랜 시간이 소요되긴 하였으나, 더 높은 정확도를 확인할 수 있었다.\")       # <= 이 부분에 답안을 작성"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "308.965px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}